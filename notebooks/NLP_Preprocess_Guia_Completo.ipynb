{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cba8711e",
   "metadata": {},
   "source": [
    "# Guia Prático de Pré-processamento de Texto (PT-BR) — Versão Simplificada\n",
    "\n",
    "Este notebook é um guia didático para aprender **NLP aplicado à classificação de texto**.  \n",
    "Etapas incluídas:\n",
    "\n",
    "1. Exploração inicial dos dados (EDA)\n",
    "2. Limpeza e normalização de texto\n",
    "3. Tokenização, stopwords, stemming e lematização\n",
    "4. Frequência de palavras\n",
    "5. Representação com TF–IDF\n",
    "6. Treino/teste com Naive Bayes (baseline)\n",
    "\n",
    "---\n",
    "\n",
    "### Conceitos-chave\n",
    "\n",
    "- **Tokenização** → dividir o texto em palavras.  \n",
    "- **Stopwords** → palavras muito comuns que não ajudam na classificação (ex.: \"de\", \"a\", \"o\").  \n",
    "- **Stemming** → reduz a palavra para uma raiz (ex.: \"agredindo\" → \"agred\").  \n",
    "- **Lematização** → reduz para a forma dicionário (ex.: \"agredindo\" → \"agredir\").  \n",
    "- **n-grams** → combinações de n palavras consecutivas (1=unigram, 2=bigram).  \n",
    "- **TF–IDF** → transforma texto em vetores.  \n",
    "  - TF = frequência de um termo no documento  \n",
    "  - IDF = quão raro é no corpus  \n",
    "  - resultado: destaca termos que são **característicos** de cada classe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690562f2",
   "metadata": {},
   "source": [
    "## 0) Instalação (se necessário)\n",
    "\n",
    "Se estiver no Google Colab, descomente e rode:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy nltk unidecode scikit-learn\n",
    "# !python -m spacy download pt_core_news_sm\n",
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98779dfa",
   "metadata": {},
   "source": [
    "## 1) Imports e Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bc692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from unidecode import unidecode\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# carregar modelo spaCy em português\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# stopwords e stemmer do NLTK\n",
    "PT_STOPWORDS = set(stopwords.words(\"portuguese\"))\n",
    "STEMMER = RSLPStemmer()\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,4)\n",
    "print(\"Ambiente pronto!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e12ae1",
   "metadata": {},
   "source": [
    "## 2) Carregar Dados de Exemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce650afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini dataset fictício (substitua pelo seu CSV real)\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"A vítima relatou ameaça grave por parte do companheiro via telefone.\",\n",
    "        \"Durante a discussão, ocorreu lesão corporal com empurrões e tapas.\",\n",
    "        \"O autor enviou mensagens intimidatórias, dizendo que iria machucar a vítima.\",\n",
    "        \"Foi registrado boletim por agressão física, resultando em hematomas no braço.\",\n",
    "        \"Sem indícios claros de crime, apenas desentendimento verbal entre as partes.\"\n",
    "    ],\n",
    "    \"label\": [\"AMEAÇA\", \"LESÃO CORPORAL\", \"AMEAÇA\", \"LESÃO CORPORAL\", \"OUTRO\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776b802b",
   "metadata": {},
   "source": [
    "## 3) EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb459f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Info:\")\n",
    "display(df.info())\n",
    "print(\"\\nDistribuição de classes:\")\n",
    "display(df['label'].value_counts())\n",
    "\n",
    "# tamanho dos textos\n",
    "df['n_chars'] = df['text'].apply(len)\n",
    "df['n_words'] = df['text'].apply(lambda x: len(x.split()))\n",
    "display(df[['n_chars','n_words']].describe())\n",
    "\n",
    "plt.hist(df['n_words'], bins=10)\n",
    "plt.title(\"Distribuição de tamanho dos textos (n_words)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956efbc",
   "metadata": {},
   "source": [
    "## 4) Limpeza de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f58f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    t = text.lower()\n",
    "    t = re.sub(r\"(https?://\\S+|www\\.\\S+)\", \" \", t)  # urls\n",
    "    t = re.sub(r\"\\b[\\w.-]+@[\\w.-]+\\.\\w{2,}\\b\", \" \", t)  # emails\n",
    "    t = re.sub(r\"@\\w+\", \" \", t)  # menções\n",
    "    t = re.sub(r\"\\d+\", \" \", t)  # números\n",
    "    t = t.translate(str.maketrans(\"\", \"\", string.punctuation))  # pontuação\n",
    "    t = unidecode(t)  # acentos\n",
    "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "df['clean'] = df['text'].apply(clean_text)\n",
    "df[['text','clean']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9eda0c",
   "metadata": {},
   "source": [
    "## 5) Tokenização, Stopwords, Stemming e Lematização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9abc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): return text.split()\n",
    "def remove_stop(tokens): return [t for t in tokens if t not in PT_STOPWORDS]\n",
    "def stem_tokens(tokens): return [STEMMER.stem(t) for t in tokens]\n",
    "def lemmatize(text): return [tok.lemma_ for tok in nlp(text)]\n",
    "\n",
    "df['tokens'] = df['clean'].apply(tokenize)\n",
    "df['nostop'] = df['tokens'].apply(remove_stop)\n",
    "df['stem'] = df['nostop'].apply(stem_tokens)\n",
    "df['lemmas'] = df['clean'].apply(lemmatize)\n",
    "\n",
    "df[['clean','nostop','stem','lemmas']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0fffb14",
   "metadata": {},
   "source": [
    "## 6) Frequência de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d53dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def top_freq(token_lists, k=10):\n",
    "    counts = Counter(chain.from_iterable(token_lists))\n",
    "    return pd.DataFrame(counts.most_common(k), columns=['token','freq'])\n",
    "\n",
    "top_freq(df['nostop'], k=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b0b90b",
   "metadata": {},
   "source": [
    "## 7) Representação com TF–IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe18addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df['clean'].tolist()\n",
    "y = df['label'].tolist()\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=list(PT_STOPWORDS), ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(X_text)\n",
    "print(\"Shape da matriz TF-IDF:\", X_tfidf.shape)\n",
    "\n",
    "# termos com maior IDF (mais raros)\n",
    "idf_sorted = sorted(zip(tfidf.idf_, tfidf.get_feature_names_out()), reverse=True)[:10]\n",
    "pd.DataFrame(idf_sorted, columns=['idf','termo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442397ea",
   "metadata": {},
   "source": [
    "## 8) Baseline com Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af66847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_text, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=list(PT_STOPWORDS), ngram_range=(1,2))),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "preds = pipe.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, preds))\n",
    "cm = confusion_matrix(y_val, preds, labels=sorted(set(y)))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=sorted(set(y)))\n",
    "disp.plot(values_format='d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cedf17",
   "metadata": {},
   "source": [
    "## 9) Próximos Passos\n",
    "\n",
    "- Testar Logistic Regression e SVM\n",
    "- Balancear classes\n",
    "- Usar embeddings (BERT, LegalBERT-pt)\n",
    "- Documentar sempre as escolhas!"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
